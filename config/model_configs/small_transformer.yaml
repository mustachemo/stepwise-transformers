# Small transformer configuration for faster training and testing
# @package model

d_model: 256
n_heads: 4
n_encoder_layers: 4
n_decoder_layers: 4
d_ff: 1024
dropout: 0.1
src_vocab_size: 16000
tgt_vocab_size: 16000
max_seq_length: 256

# Transformer Implementation Guide

## Core Components Implementation Order

### 1. Positional Encoding
```python
class PositionalEncoding(nn.Module):
    """Implements sinusoidal positional encoding as described in 'Attention Is All You Need'."""

    def __init__(self, d_model: int, max_len: int = 5000):
        """
        Args:
            d_model: Embedding dimension
            max_len: Maximum sequence length
        """
```

**Key Points:**
- Use sine and cosine functions with different frequencies
- Even dimensions use sine, odd dimensions use cosine
- Allows model to learn relative positions

### 2. Multi-Head Attention
```python
class MultiHeadAttention(nn.Module):
    """Implements multi-head attention mechanism."""

    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):
        """
        Args:
            d_model: Model dimension
            num_heads: Number of attention heads
            dropout: Dropout probability
        """
```

**Mathematical Implementation:**
- Split d_model into num_heads heads
- Compute attention for each head: Attention(Q,K,V) = softmax(QK^T/√d_k)V
- Concatenate outputs and apply linear transformation
- Use scaled dot-product attention

### 3. Feed-Forward Network
```python
class FeedForward(nn.Module):
    """Implements the feed-forward network with two linear transformations."""

    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):
        """
        Args:
            d_model: Model dimension
            d_ff: Feed-forward dimension (typically 4 * d_model)
            dropout: Dropout probability
        """
```

### 4. Encoder Block
```python
class EncoderBlock(nn.Module):
    """Transformer encoder block with self-attention and feed-forward layers."""

    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):
        """
        Args:
            d_model: Model dimension
            num_heads: Number of attention heads
            d_ff: Feed-forward dimension
            dropout: Dropout probability
        """
```

**Architecture:**
- Multi-head self-attention
- Add & Norm (residual connection + layer normalization)
- Feed-forward network
- Add & Norm (residual connection + layer normalization)

### 5. Decoder Block
```python
class DecoderBlock(nn.Module):
    """Transformer decoder block with masked self-attention, encoder-decoder attention, and feed-forward."""
```

**Key Differences from Encoder:**
- Masked self-attention (prevents looking at future tokens)
- Encoder-decoder attention layer
- Three sub-layers instead of two

## Mathematical Formulas to Implement

### Attention Mechanism
```
Attention(Q,K,V) = softmax(QK^T/√d_k)V
```

### Multi-Head Attention
```
MultiHead(Q,K,V) = Concat(head_1,...,head_h)W^O
where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

### Positional Encoding
```
PE(pos,2i) = sin(pos/10000^(2i/d_model))
PE(pos,2i+1) = cos(pos/10000^(2i/d_model))
```

## Implementation Best Practices

1. **Gradient Flow**: Always use residual connections
2. **Normalization**: Apply layer normalization before sub-layers
3. **Dropout**: Apply dropout after attention and feed-forward
4. **Masking**: Implement proper masking for decoder
5. **Scaling**: Always scale attention scores by √d_k
6. **Initialization**: Use proper weight initialization for linear layers

## Testing Strategy
- Test each component with simple examples
- Verify attention weights sum to 1
- Check that positional encoding has expected properties
- Test masking prevents looking at future tokens
- Verify gradient flow through residual connections
description:
globs:
alwaysApply: false
---

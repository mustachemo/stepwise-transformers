# MLflow Experiment Guide for Transformers Learning

## Application Architecture

### Main Experiment Structure
```python
"""Interactive transformer learning application built with MLflow.

This application provides an educational environment for learning transformer architecture,
attention mechanisms, and neural network concepts through MLflow experiments,
interactive visualization, and hands-on experimentation.
"""
from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Optional, An

import mlflow
import mlflow.pytorch
import torch
import torch.nn as nn
from loguru import logger
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn

# =============================== Constants =================================== #
DEFAULT_EXPERIMENT_NAME: str = "transformer_learning"
DEFAULT_TRACKING_URI: str = "sqlite:///mlruns.db"
DEFAULT_ARTIFACT_PATH: str = "mlruns"

# =============================== Configuration =============================== #
# * Use Loguru for robust logging with file rotation and structured output.
logger.add(
    "logs/transformer_experiments_{time}.log",
    level="INFO",
    rotation="10 MB",
    retention="10 days",
    format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} - {message}",
)

# =============================== Data Structures ============================= #
@dataclass
class ExperimentConfig:
    """Configuration for transformer learning experiments."""

    experiment_name: str
    tracking_uri: str
    artifact_path: str
    log_level: str
    auto_save: bool
    enable_ui: bool


# ============================== Main Application ============================= #
class TransformerExperimentManager:
    """Main application for interactive transformer learning with MLflow.

    This application provides a comprehensive environment for learning transformer
    architecture through MLflow experiments, interactive visualization,
    and hands-on experimentation.
    """

    def __init__(self, config_path: Optional[Path] = None) -> None:
        """Initialize the transformer learning experiment manager.

        Args:
            config_path: Path to configuration file. If None, uses default.

        Raises:
            FileNotFoundError: If configuration file is specified but not found.
            OSError: If application directories cannot be created.
        """
        self.config_path = config_path or Path("config.json")
        self.experiment_config: Optional[ExperimentConfig] = None
        self.console = Console()
        self.current_experiment: Optional[mlflow.entities.Experiment] = None

        try:
            self._load_configuration()
            self._setup_mlflow()
            self._setup_directories()
            logger.info("TransformerExperimentManager initialized successfully")
        except (FileNotFoundError, OSError) as exc:
            logger.error(f"Failed to initialize TransformerExperimentManager: {exc}")
            raise

    def _load_configuration(self) -> None:
        """Load application configuration from file.

        Raises:
            FileNotFoundError: If configuration file is not found.
            ValueError: If configuration file is malformed.
        """
        if self.config_path.exists():
            try:
                # TODO: Implement configuration loading logic
                self.experiment_config = ExperimentConfig(
                    experiment_name=DEFAULT_EXPERIMENT_NAME,
                    tracking_uri=DEFAULT_TRACKING_URI,
                    artifact_path=DEFAULT_ARTIFACT_PATH,
                    log_level="INFO",
                    auto_save=True,
                    enable_ui=True,
                )
                logger.info(f"Configuration loaded from {self.config_path}")
            except Exception as exc:
                logger.error(f"Failed to load configuration: {exc}")
                raise ValueError(f"Malformed configuration file: {exc}")
        else:
            # * Use default configuration if file doesn't exist
            self.experiment_config = ExperimentConfig(
                experiment_name=DEFAULT_EXPERIMENT_NAME,
                tracking_uri=DEFAULT_TRACKING_URI,
                artifact_path=DEFAULT_ARTIFACT_PATH,
                log_level="INFO",
                auto_save=True,
                enable_ui=True,
            )
            logger.info("Using default configuration")

    def _setup_mlflow(self) -> None:
        """Set up MLflow tracking and experiment management.

        Raises:
            RuntimeError: If MLflow setup fails.
        """
        try:
            mlflow.set_tracking_uri(self.experiment_config.tracking_uri)
            mlflow.set_experiment(self.experiment_config.experiment_name)
            self.current_experiment = mlflow.get_experiment_by_name(
                self.experiment_config.experiment_name
            )
            logger.info("MLflow setup completed successfully")
        except Exception as exc:
            logger.error(f"Failed to setup MLflow: {exc}")
            raise RuntimeError(f"MLflow setup failed: {exc}")

    def _setup_directories(self) -> None:
        """Create necessary application directories.

        Raises:
            OSError: If directories cannot be created.
        """
        try:
            Path("logs").mkdir(exist_ok=True)
            Path("experiments").mkdir(exist_ok=True)
            Path("data").mkdir(exist_ok=True)
            Path("models").mkdir(exist_ok=True)
            Path("artifacts").mkdir(exist_ok=True)
            logger.info("Application directories created successfully")
        except OSError as exc:
            logger.error(f"Failed to create application directories: {exc}")
            raise

    def start_new_experiment(self, experiment_name: str, description: str = "") -> str:
        """Start a new MLflow experiment.

        Args:
            experiment_name: Name of the experiment.
            description: Optional description of the experiment.

        Returns:
            str: The experiment ID.

        Raises:
            RuntimeError: If experiment creation fails.
        """
        try:
            with mlflow.start_run(run_name=experiment_name) as run:
                mlflow.log_param("description", description)
                mlflow.log_param("experiment_type", "transformer_learning")
                mlflow.log_param("timestamp", str(mlflow.start_time))

                logger.info(f"Started new experiment: {experiment_name}")
                return run.info.run_id
        except Exception as exc:
            logger.error(f"Failed to start experiment: {exc}")
            raise RuntimeError(f"Experiment creation failed: {exc}")

    def log_transformer_component(self, component_name: str, parameters: dict[str, Any]) -> None:
        """Log transformer component parameters and metrics.

        Args:
            component_name: Name of the transformer component.
            parameters: dictionary of component parameters.

        Raises:
            RuntimeError: If logging fails.
        """
        try:
            for key, value in parameters.items():
                mlflow.log_param(f"{component_name}_{key}", value)

            logger.info(f"Logged {component_name} component parameters")
        except Exception as exc:
            logger.error(f"Failed to log component: {exc}")
            raise RuntimeError(f"Component logging failed: {exc}")

    def log_training_metrics(self, metrics: dict[str, float], step: int) -> None:
        """Log training metrics to MLflow.

        Args:
            metrics: dictionary of training metrics.
            step: Training step number.

        Raises:
            RuntimeError: If logging fails.
        """
        try:
            for metric_name, metric_value in metrics.items():
                mlflow.log_metric(metric_name, metric_value, step=step)

            logger.info(f"Logged training metrics at step {step}")
        except Exception as exc:
            logger.error(f"Failed to log training metrics: {exc}")
            raise RuntimeError(f"Metrics logging failed: {exc}")

    def save_model(self, model: nn.Module, model_name: str) -> None:
        """Save a PyTorch model to MLflow.

        Args:
            model: The PyTorch model to save.
            model_name: Name for the saved model.

        Raises:
            RuntimeError: If model saving fails.
        """
        try:
            mlflow.pytorch.log_model(model, model_name)
            logger.info(f"Saved model: {model_name}")
        except Exception as exc:
            logger.error(f"Failed to save model: {exc}")
            raise RuntimeError(f"Model saving failed: {exc}")

    def log_attention_visualization(self, attention_weights: torch.Tensor, step: int) -> None:
        """Log attention weight visualization.

        Args:
            attention_weights: Attention weight tensor.
            step: Training step number.

        Raises:
            RuntimeError: If visualization logging fails.
        """
        try:
            # Convert attention weights to image and log as artifact
            attention_path = f"artifacts/attention_step_{step}.png"
            # TODO: Implement attention weight visualization
            mlflow.log_artifact(attention_path, f"attention_visualizations/step_{step}")

            logger.info(f"Logged attention visualization for step {step}")
        except Exception as exc:
            logger.error(f"Failed to log attention visualization: {exc}")
            raise RuntimeError(f"Visualization logging failed: {exc}")

    def list_experiments(self) -> list[mlflow.entities.Experiment]:
        """list all available experiments.

        Returns:
            list[mlflow.entities.Experiment]: list of experiments.

        Raises:
            RuntimeError: If experiment listing fails.
        """
        try:
            experiments = mlflow.search_experiments()
            logger.info(f"Found {len(experiments)} experiments")
            return experiments
        except Exception as exc:
            logger.error(f"Failed to list experiments: {exc}")
            raise RuntimeError(f"Experiment listing failed: {exc}")

    def compare_experiments(self, experiment_ids: list[str]) -> None:
        """Compare multiple experiments.

        Args:
            experiment_ids: list of experiment IDs to compare.

        Raises:
            RuntimeError: If comparison fails.
        """
        try:
            # TODO: Implement experiment comparison logic
            logger.info(f"Comparing {len(experiment_ids)} experiments")
        except Exception as exc:
            logger.error(f"Failed to compare experiments: {exc}")
            raise RuntimeError(f"Experiment comparison failed: {exc}")

    def export_experiment_results(self, experiment_id: str, output_path: Path) -> None:
        """Export experiment results to file.

        Args:
            experiment_id: ID of the experiment to export.
            output_path: Path where to save the results.

        Raises:
            RuntimeError: If export fails.
        """
        try:
            # TODO: Implement experiment export logic
            logger.info(f"Exported experiment {experiment_id} to {output_path}")
        except Exception as exc:
            logger.error(f"Failed to export experiment: {exc}")
            raise RuntimeError(f"Experiment export failed: {exc}")
```

## Experiment Components

### 1. Attention Mechanism Experiments
```python
class AttentionExperiment:
    """Interactive attention mechanism experiments."""

    def __init__(self, experiment_manager: TransformerExperimentManager):
        self.experiment_manager = experiment_manager
        self.console = Console()

    def run_single_head_attention(self, query: torch.Tensor, key: torch.Tensor,
                                 value: torch.Tensor) -> torch.Tensor:
        """Run single-head attention experiment.

        Args:
            query: Query tensor.
            key: Key tensor.
            value: Value tensor.

        Returns:
            torch.Tensor: Attention output.

        Raises:
            RuntimeError: If attention computation fails.
        """
        try:
            with mlflow.start_run(run_name="single_head_attention") as run:
                # Log input parameters
                mlflow.log_param("query_shape", list(query.shape))
                mlflow.log_param("key_shape", list(key.shape))
                mlflow.log_param("value_shape", list(value.shape))

                # Compute attention
                attention_scores = torch.matmul(query, key.transpose(-2, -1))
                attention_weights = torch.softmax(attention_scores, dim=-1)
                output = torch.matmul(attention_weights, value)

                # Log attention weights visualization
                self.experiment_manager.log_attention_visualization(
                    attention_weights, step=0
                )

                # Log metrics
                mlflow.log_metric("attention_entropy",
                                -torch.sum(attention_weights * torch.log(attention_weights + 1e-8)))
                mlflow.log_metric("output_norm", torch.norm(output))

                logger.info("Single-head attention experiment completed")
                return output

        except Exception as exc:
            logger.error(f"Single-head attention experiment failed: {exc}")
            raise RuntimeError(f"Attention experiment failed: {exc}")

    def run_multi_head_attention(self, num_heads: int, d_model: int,
                                query: torch.Tensor, key: torch.Tensor,
                                value: torch.Tensor) -> torch.Tensor:
        """Run multi-head attention experiment.

        Args:
            num_heads: Number of attention heads.
            d_model: Model dimension.
            query: Query tensor.
            key: Key tensor.
            value: Value tensor.

        Returns:
            torch.Tensor: Multi-head attention output.

        Raises:
            RuntimeError: If multi-head attention computation fails.
        """
        try:
            with mlflow.start_run(run_name="multi_head_attention") as run:
                # Log parameters
                mlflow.log_param("num_heads", num_heads)
                mlflow.log_param("d_model", d_model)
                mlflow.log_param("query_shape", list(query.shape))
                mlflow.log_param("key_shape", list(key.shape))
                mlflow.log_param("value_shape", list(value.shape))

                # Split into multiple heads
                d_k = d_model // num_heads
                query_heads = query.view(-1, num_heads, d_k)
                key_heads = key.view(-1, num_heads, d_k)
                value_heads = value.view(-1, num_heads, d_k)

                # Compute attention for each head
                attention_outputs = []
                for head in range(num_heads):
                    head_output = self.run_single_head_attention(
                        query_heads[:, head], key_heads[:, head], value_heads[:, head]
                    )
                    attention_outputs.append(head_output)

                # Concatenate head outputs
                output = torch.cat(attention_outputs, dim=-1)

                # Log multi-head metrics
                mlflow.log_metric("num_heads_used", num_heads)
                mlflow.log_metric("output_norm", torch.norm(output))

                logger.info("Multi-head attention experiment completed")
                return output

        except Exception as exc:
            logger.error(f"Multi-head attention experiment failed: {exc}")
            raise RuntimeError(f"Multi-head attention experiment failed: {exc}")
```

### 2. Positional Encoding Experiments
```python
class PositionalEncodingExperiment:
    """Interactive positional encoding experiments."""

    def __init__(self, experiment_manager: TransformerExperimentManager):
        self.experiment_manager = experiment_manager
        self.console = Console()

    def run_sinusoidal_encoding(self, seq_length: int, d_model: int) -> torch.Tensor:
        """Run sinusoidal positional encoding experiment.

        Args:
            seq_length: Length of the sequence.
            d_model: Model dimension.

        Returns:
            torch.Tensor: Positional encoding tensor.

        Raises:
            RuntimeError: If encoding computation fails.
        """
        try:
            with mlflow.start_run(run_name="sinusoidal_positional_encoding") as run:
                # Log parameters
                mlflow.log_param("seq_length", seq_length)
                mlflow.log_param("d_model", d_model)

                # Create positional encoding
                pe = torch.zeros(seq_length, d_model)
                position = torch.arange(0, seq_length).unsqueeze(1).float()
                div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                                   -(torch.log(torch.tensor(10000.0)) / d_model))

                pe[:, 0::2] = torch.sin(position * div_term)
                pe[:, 1::2] = torch.cos(position * div_term)

                # Log metrics
                mlflow.log_metric("encoding_norm", torch.norm(pe))
                mlflow.log_metric("encoding_std", torch.std(pe))

                logger.info("Sinusoidal positional encoding experiment completed")
                return pe

        except Exception as exc:
            logger.error(f"Sinusoidal positional encoding experiment failed: {exc}")
            raise RuntimeError(f"Positional encoding experiment failed: {exc}")
```

### 3. Transformer Block Experiments
```python
class TransformerBlockExperiment:
    """Interactive transformer block experiments."""

    def __init__(self, experiment_manager: TransformerExperimentManager):
        self.experiment_manager = experiment_manager
        self.console = Console()

    def run_encoder_block(self, input_tensor: torch.Tensor,
                         d_model: int, num_heads: int, d_ff: int,
                         dropout: float = 0.1) -> torch.Tensor:
        """Run transformer encoder block experiment.

        Args:
            input_tensor: Input tensor.
            d_model: Model dimension.
            num_heads: Number of attention heads.
            d_ff: Feed-forward dimension.
            dropout: Dropout rate.

        Returns:
            torch.Tensor: Encoder block output.

        Raises:
            RuntimeError: If encoder block computation fails.
        """
        try:
            with mlflow.start_run(run_name="transformer_encoder_block") as run:
                # Log parameters
                mlflow.log_param("d_model", d_model)
                mlflow.log_param("num_heads", num_heads)
                mlflow.log_param("d_ff", d_ff)
                mlflow.log_param("dropout", dropout)
                mlflow.log_param("input_shape", list(input_tensor.shape))

                # Multi-head attention
                attention_output = self._multi_head_attention(
                    input_tensor, d_model, num_heads, dropout
                )

                # Add & Norm
                residual_output = input_tensor + attention_output
                normalized_output = self._layer_norm(residual_output, d_model)

                # Feed-forward network
                ff_output = self._feed_forward(normalized_output, d_model, d_ff, dropout)

                # Add & Norm
                final_output = normalized_output + ff_output
                output = self._layer_norm(final_output, d_model)

                # Log metrics
                mlflow.log_metric("output_norm", torch.norm(output))
                mlflow.log_metric("residual_norm", torch.norm(residual_output))

                logger.info("Transformer encoder block experiment completed")
                return output

        except Exception as exc:
            logger.error(f"Transformer encoder block experiment failed: {exc}")
            raise RuntimeError(f"Encoder block experiment failed: {exc}")

    def _multi_head_attention(self, x: torch.Tensor, d_model: int,
                             num_heads: int, dropout: float) -> torch.Tensor:
        """Compute multi-head attention."""
        # Implementation details
        pass

    def _feed_forward(self, x: torch.Tensor, d_model: int,
                      d_ff: int, dropout: float) -> torch.Tensor:
        """Compute feed-forward network."""
        # Implementation details
        pass

    def _layer_norm(self, x: torch.Tensor, d_model: int) -> torch.Tensor:
        """Compute layer normalization."""
        # Implementation details
        pass
```

## Interactive Features

### 1. Real-time Experiment Monitoring
- Live metric tracking with MLflow
- Interactive progress bars
- Real-time visualization updates
- Experiment comparison tools

### 2. Component Testing
- Interactive parameter adjustment
- Real-time model evaluation
- Performance metrics display
- Comparison tools

### 3. Data Visualization
- Attention weight heatmaps
- Training loss curves
- Model architecture diagrams
- Component interaction graphs

### 4. Progress Tracking
- Learning module completion
- Achievement system
- Skill assessment
- Progress export

## Experiment Management

### Experiment Storage
```python
@dataclass
class Experiment:
    """Represents a saved experiment."""
    name: str
    timestamp: datetime
    model_config: ModelConfig
    training_config: TrainingConfig
    results: dict[str, Any]
    notes: str
```

### Progress Tracking
```python
@dataclass
class LearningProgress:
    """Tracks user learning progress."""
    completed_modules: Set[str]
    current_module: str
    achievements: list[Achievement]
    assessment_scores: dict[str, float]
```

## Integration with Transformer Components

### Real-time Model Updates
- Live parameter adjustment
- Instant model recompilation
- Real-time performance metrics
- Interactive debugging

### Visualization Integration
- Attention weight display
- Gradient flow visualization
- Model architecture diagrams
- Training progress charts

### Export Capabilities
- Model configuration export
- Training results export
- Visualization export
- Progress report generation
description:
globs:
alwaysApply: false
---
